{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import random\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.channel_grouping import load_backbone_model, channel_grouping_layer\n",
    "from networks.dgcn import dgcn_cls\n",
    "from networks.loss import channel_grouping_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset_ImagePrivacy import IPDataset_FromFileList, full_transform\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 1\n",
    "partition = str(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/image_privacy/'\n",
    "train_images = data_dir + 'exp/partition'+ partition + '/train.csv'\n",
    "val_images = data_dir + 'exp/partition'+ partition + '/val.csv'\n",
    "test_images = data_dir + 'exp/partition'+ partition + '/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = IPDataset_FromFileList(train_images, full_transform)\n",
    "val_data = IPDataset_FromFileList(val_images,full_transform)\n",
    "test_data = IPDataset_FromFileList(test_images, full_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with the unbalanced dataset\n",
    "\n",
    "private_nums, public_nums = train_data.labels.count(0), train_data.labels.count(1)\n",
    "sample_class_count  = torch.Tensor([private_nums, public_nums])\n",
    "\n",
    "class_weight = sample_class_count.float() /train_data.__len__()\n",
    "class_weight = 1.-class_weight\n",
    "\n",
    "class_weight = class_weight.to(device)\n",
    "print(class_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_nums = list(range(2,14,2))\n",
    "part_num = part_nums[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model_path = './models/ResNet4IP.pth'\n",
    "checkpoint_dir = './models/ImagePrivacy/'\n",
    "checkpoint_dir = checkpoint_dir + str(part_num)\n",
    "channel_grouping_layer_path = checkpoint_dir + '/channel_grouping_layer.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_model = load_backbone_model(backbone_model_path).to(device) # pretrained feature extractor\n",
    "channel_grouping = channel_grouping_layer(part_num=part_num, channel_num=2048).to(device)\n",
    "channel_grouping.load_state_dict(torch.load(channel_grouping_layer_path)) # pretrained channel grouping layer\n",
    "dgcn = dgcn_cls(part_num=part_num).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_backbone = 1e-5\n",
    "learning_rate_cls = 1e-5\n",
    "learning_rate_cgl = 1e-3\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_loss = nn.CrossEntropyLoss(weight=class_weight)\n",
    "cgl_loss = channel_grouping_loss()\n",
    "\n",
    "# cls_optimizer = torch.optim.SGD(dgcn.parameters(), lr=learning_rate, momentum=momentum)\n",
    "# cgl_optimizer = torch.optim.SGD(channel_grouping.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "backbone_optimizer = torch.optim.Adam(backbone_model.parameters(), lr=learning_rate_backbone,weight_decay=weight_decay)\n",
    "cls_optimizer = torch.optim.Adam(dgcn.parameters(), lr=learning_rate_cls,weight_decay=weight_decay)\n",
    "cgl_optimizer = torch.optim.Adam(channel_grouping.parameters(), lr=learning_rate_cgl,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_backbone = ReduceLROnPlateau(backbone_optimizer, mode='min', factor=0.1, patience=1)\n",
    "scheduler_cls = ReduceLROnPlateau(cls_optimizer, mode='min', factor=0.1, patience=1)\n",
    "scheduler_cgl = ReduceLROnPlateau(cgl_optimizer, mode='min', factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data_loader):\n",
    "    # validating\n",
    "    print('validating')\n",
    "    print(time.asctime())\n",
    "    \n",
    "    backbone_model.eval()\n",
    "    dgcn.eval()\n",
    "    channel_grouping.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    TP,FP,FN,TN = 0,0,0,0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "           \n",
    "            target = data[0].to(device)\n",
    "            img = data[1].to(device)\n",
    "\n",
    "            feature = backbone_model(img).reshape(-1, 2048, 14, 14)\n",
    "            grouping_result, weighted_feature = channel_grouping(feature)\n",
    "            cls_res = dgcn(feature, weighted_feature)\n",
    "\n",
    "            predicted = torch.argmax(cls_res.data,-1)\n",
    "\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            TP += ((target == 0) & (predicted == 0)).sum().item()\n",
    "            FP += ((target == 0) & (predicted == 1)).sum().item()\n",
    "            FN += ((target == 1) & (predicted == 0)).sum().item()\n",
    "            TN += ((target == 1) & (predicted == 1)).sum().item()\n",
    "\n",
    "            del(cls_res)\n",
    "            del(predicted)\n",
    "            \n",
    "    acc = 100. * correct / total\n",
    "    \n",
    "    if data_loader == test_loader:\n",
    "\n",
    "        print('testing accuracy：%.3f%%' % (acc))\n",
    "\n",
    "    else:\n",
    "        print('validating accuracy：%.3f%%' % (acc))\n",
    "\n",
    "    try:\n",
    "\n",
    "        #private metrics\n",
    "        p1 = TP / (TP + FP)\n",
    "        r1 = TP / (TP + FN)\n",
    "        f1 = (2 * p1 * r1) / (p1 + r1)\n",
    "\n",
    "        #public metrics\n",
    "        p2 = TN / (TN + FN)\n",
    "        r2 = TN / (TN + FP)\n",
    "        f2 = (2 * p2 * r2) / (p2 + r2)\n",
    "\n",
    "        print('===========================')\n",
    "\n",
    "        print('private class metrics:')\n",
    "        \n",
    "        print('precision, recall, f1:')\n",
    "        print('%.3f%%\\t%.3f%%\\t%.3f' % (p1 * 100, r1 * 100, f1))\n",
    "\n",
    "        print('===========================')\n",
    "        \n",
    "        print('public class metrics:')\n",
    "        \n",
    "        print('precision, recall, f1:')\n",
    "        print('%.3f%%\\t%.3f%%\\t%.3f' % (p2 * 100, r2 * 100, f2))\n",
    "        \n",
    "        print('===========================')\n",
    "\n",
    "\n",
    "#         print('===========================')\n",
    "#         print((TP+TN)/(TP+TN+FP+FN))\n",
    "#         print('===========================')\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint dir\n",
    "\n",
    "checkpoint_dir = checkpoint_dir + '/wo_cgl_finetune'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgl_checkpoint = checkpoint_dir + '/CGL_IP(0)_36.116_36.106.pth'\n",
    "channel_grouping.load_state_dict(torch.load(cgl_checkpoint))\n",
    "\n",
    "dgcn_checkpoint = checkpoint_dir + '/DGCN_IP(9)_86.502_86.699.pth'\n",
    "dgcn.load_state_dict(torch.load(dgcn_checkpoint))\n",
    "\n",
    "backbone_checkpoint = checkpoint_dir + '/ResNet_IP(9)_86.502_86.699.pth'\n",
    "backbone_model.load_state_dict(torch.load(backbone_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('current learning rate:')\n",
    "print('cls:')\n",
    "print(cls_optimizer.param_groups[0]['lr'])\n",
    "print('backbone:')\n",
    "print(backbone_optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_start = 1\n",
    "\n",
    "interval = 20\n",
    "\n",
    "for epoch in range(epoch_start, (epoch_start+epochs)):\n",
    "    running_loss_cls, count, acc = 0., 0, 0.\n",
    "    running_loss_dis, running_loss_div = 0., 0.\n",
    "\n",
    "    print('training')\n",
    "    print(time.asctime())\n",
    "\n",
    "    print('current learning rate:')\n",
    "    print('cls:')\n",
    "    print(cls_optimizer.param_groups[0]['lr'])\n",
    "    print('backbone:')\n",
    "    print(backbone_optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    if epoch%interval != 0:\n",
    "        dgcn.train()\n",
    "        backbone_model.train()\n",
    "        channel_grouping.eval()\n",
    "    else:\n",
    "        dgcn.eval()\n",
    "        backbone_model.eval()\n",
    "        channel_grouping.train()\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        target = data[0].to(device)\n",
    "        img = data[1].to(device)\n",
    "        \n",
    "        feature = backbone_model(img).reshape(-1, 2048, 14, 14)\n",
    "        grouping_result, weighted_feature = channel_grouping(feature)\n",
    "        cls_res = dgcn(feature, weighted_feature)\n",
    "        \n",
    "        if epoch%interval != 0:\n",
    "            backbone_optimizer.zero_grad()\n",
    "            cls_optimizer.zero_grad()\n",
    "            \n",
    "            loss = cls_loss(cls_res, target)\n",
    "            running_loss_cls += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            cls_optimizer.step()\n",
    "            \n",
    "            if epoch > 5:\n",
    "                backbone_optimizer.step()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            cgl_optimizer.zero_grad()\n",
    "            \n",
    "            loss1 = cgl_loss(weighted_feature)    # [dis_loss, div_loss]\n",
    "            loss2 = cls_loss(cls_res, target)    # classification loss\n",
    "\n",
    "            running_loss_dis += loss1[0].item()\n",
    "            running_loss_div += loss1[1].item()\n",
    "            \n",
    "            loss = (loss1[0] + loss1[1] + loss2)\n",
    "\n",
    "            loss.backward()\n",
    "            cgl_optimizer.step()\n",
    "            \n",
    "        # print statistics\n",
    "        if i % 50 == 49:\n",
    "            if epoch%interval != 0:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss_cls / (i + 1)))\n",
    "            else:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, loss2))\n",
    "                print('[%d, %5d] dis/div loss: %.8f, %.8f' %\n",
    "                      (epoch + 1, i + 1, running_loss_dis / (i + 1), running_loss_div / (i + 1)))\n",
    "                \n",
    "                running_loss_cgl = running_loss_dis / (i + 1) + running_loss_div / (i + 1)\n",
    "                scheduler_cgl.step(running_loss_cgl)\n",
    "\n",
    "\n",
    "    val_acc = validate(val_loader)\n",
    "    test_acc = validate(test_loader)\n",
    "\n",
    "    val_acc = round(val_acc,3)\n",
    "    test_acc = round(test_acc,3)\n",
    "\n",
    "    if epoch%interval != 0:\n",
    "        print('saving cls checkpoints....')\n",
    "\n",
    "        scheduler_cls.step(val_acc)\n",
    "        \n",
    "        if epoch>5:\n",
    "            scheduler_backbone.step(val_acc)\n",
    "            backbone_path = checkpoint_dir + '/ResNet_IP({})_{}_{}.pth'.format(epoch, val_acc, test_acc)\n",
    "            torch.save(backbone_model.state_dict(), backbone_path)\n",
    "\n",
    "        dgcn_path = checkpoint_dir + '/DGCN_IP({})_{}_{}.pth'.format(epoch, val_acc, test_acc)\n",
    "        torch.save(dgcn.state_dict(), dgcn_path)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print('saving cgl checkpoints....')\n",
    "\n",
    "#         running_loss_cgl = running_loss_dis + running_loss_div\n",
    "#         scheduler_cgl.step(running_loss_cgl)\n",
    "        \n",
    "        cgl_path = checkpoint_dir + '/CGL_IP({})_{}_{}.pth'.format(epoch, val_acc, test_acc)\n",
    "        torch.save(channel_grouping.state_dict(), cgl_path)    \n",
    "    \n",
    "            \n",
    "print('Finished Training')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgl_checkpoint = './models/channel_grouping_layer.pth'\n",
    "\n",
    "channel_grouping.load_state_dict(torch.load(cgl_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgcn_checkpoint = './models/DGCN_IP(1)_90.625_87.5.pth'\n",
    "dgcn.load_state_dict(torch.load(dgcn_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
