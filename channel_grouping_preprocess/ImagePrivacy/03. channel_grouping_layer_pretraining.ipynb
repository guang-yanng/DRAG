{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-train the channel-grouping-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import models\n",
    "from networks.channel_grouping import channel_grouping_layer, load_backbone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset_ImagePrivacy import IPDataset_FromFileList, full_transform\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 partitions in total for cross validation\n",
    "partition = 1\n",
    "partition = str(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../../data/image_privacy/exp/'\n",
    "train_images = data_dir + 'partition'+ partition + '/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = IPDataset_FromFileList(train_images, full_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the cluster label for pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the channel label, 2048 channels with peak cordinate of all the n training images\n",
    "# formated as 2048 * [tx1, ty1, tx2, ty2, ... txn, tyn]\n",
    "# the clustering result is a 2048-dimension vector for each part and will be used to supervise the fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_nums = list(range(2,14,2))\n",
    "part_num = part_nums[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_index = np.load(file='./grouping_result/channel_cluster_' + str(part_num) + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_label(part_indexs, part_num):\n",
    "    cluster_label = []\n",
    "    for i in range(part_num):\n",
    "        cluster_label.append([])\n",
    "\n",
    "    for index in part_indexs:\n",
    "        for j in range(part_num):\n",
    "            cluster_label[j].append(0)\n",
    "        cluster_label[index][-1] = 1\n",
    "    \n",
    "    return cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_label = index_to_label(part_index, part_num)\n",
    "cluster_label = np.array(cluster_label)\n",
    "cluster_label = torch.LongTensor(cluster_label).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone model to extract convolutional features.\n",
    "# the features are flattened and need to be reshaped before next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../../models/ResNet4IP.pth'\n",
    "conv_model = load_backbone_model(model_path).to(device)\n",
    "conv_model.eval()\n",
    "cgl = channel_grouping_layer(part_num=part_num, channel_num=2048).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained channel_grouping model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cgl_path = '../models/channel_grouping_layer.pth'\n",
    "# cgl.load_state_dict(torch.load(cgl_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experimental setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "learning_rate = 1e-4\n",
    "\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_result: [8(part_num) * 2048(indicater for each channel)], use MSELoss rather than CrossEntropy\n",
    "class_weight = torch.tensor([1/8,7/8])\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(class_weight).to(device)\n",
    "# loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(cgl.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# optimizer = torch.optim.SGD(cgl.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_grouping_loss(grouping_result, target, avg = True):\n",
    "    grouping_loss = torch.zeros(1).to(device)\n",
    "    \n",
    "    grouping_result = grouping_result.unsqueeze(-1)\n",
    "    res_tmp = 1. - grouping_result\n",
    "\n",
    "    grouping_label = torch.cat((grouping_result, res_tmp), dim = -1)\n",
    "\n",
    "    for i in range(grouping_label.shape[0]):\n",
    "        for j in range(target.shape[0]):\n",
    "            loss = loss_func(grouping_label[i,j,:,:], target[j])\n",
    "            grouping_loss += loss\n",
    "            \n",
    "    if avg:\n",
    "        sample_num = grouping_label.shape[0] * grouping_label.shape[1]\n",
    "        grouping_loss = grouping_loss / sample_num\n",
    "    return grouping_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data_loader):\n",
    "\n",
    "    print('validating')\n",
    "    cgl.eval()\n",
    "    running_loss, count = 0., 0\n",
    "    print(time.asctime())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            target = cluster_label\n",
    "            img = data[1].to(device)\n",
    "\n",
    "            conv_features = conv_model(img).reshape(-1, 2048, 14, 14)\n",
    "            channel_grouping_res = cgl(conv_features)\n",
    "\n",
    "            grouping_result, attention_mask = channel_grouping_res[0], channel_grouping_res[1]\n",
    "\n",
    "            loss = cal_grouping_loss(grouping_result, target)\n",
    "\n",
    "            # print statistics\n",
    "            count += data[0].shape[0]\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = running_loss / count\n",
    "    avg_loss = running_loss / (i+1)\n",
    "\n",
    "    print(\"avg_loss:\" + str(avg_loss))\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '../../models/ImagePrivacy/'\n",
    "\n",
    "checkpoint_dir = checkpoint_dir + str(part_num)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# cgl_path = checkpoint_dir + '/channel_grouping_layer(0)_0.313261658.pth'\n",
    "# cgl.load_state_dict(torch.load(cgl_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_start = 0\n",
    "for epoch in range(epoch_start, epoch_start + epochs):\n",
    "    print('training')\n",
    "    cgl.train()\n",
    "    running_loss, count, acc = 0., 0, 0.\n",
    "    print(time.asctime())\n",
    "    \n",
    "    print('current learning rate:')\n",
    "    print(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        target = cluster_label\n",
    "        img = data[1].to(device)\n",
    "        \n",
    "        #use the torchvision model for convinence, but should reshape to deal with the flatten layer\n",
    "        conv_features = conv_model(img).reshape(-1, 2048, 14, 14)\n",
    "        channel_grouping_res = cgl(conv_features)\n",
    "        \n",
    "        grouping_result, attention_mask = channel_grouping_res[0], channel_grouping_res[1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = cal_grouping_loss(grouping_result, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # print statistics     \n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every several mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / (i + 1)))\n",
    "\n",
    "    avg_loss = validate(train_loader)    \n",
    "    avg_loss = round(avg_loss,9)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    model_path = checkpoint_dir + '/channel_grouping_layer({})_{}.pth'.format(epoch, avg_loss)\n",
    "    torch.save(cgl.state_dict(), model_path)\n",
    "\n",
    "            \n",
    "print('Finished Training')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validate(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
